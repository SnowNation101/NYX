{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Downloaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset = []\n",
    "with open(\"/fs/archive/share/mm_datasets/obelics_processed.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)  # Parse the JSON line into a dictionary\n",
    "        dataset.append(data)  # Append each parsed dictionary to the dataset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from transformers import AutoProcessor\n",
    "# processor = AutoProcessor.from_pretrained(\"/fs/archive/share/Qwen2.5-VL-7B-Instruct\")\n",
    "# tokenizer = processor.tokenizer\n",
    "\n",
    "# text_lengths = [len(tokenizer.encode(data['text'])) for data in dataset]\n",
    "\n",
    "# # Plot the histogram\n",
    "# plt.hist(text_lengths, bins=50, alpha=0.75, color='skyblue', edgecolor='black')\n",
    "# plt.xlabel('Token Count')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Token Counts in Dataset')\n",
    "# plt.grid(axis='y', alpha=0.75)\n",
    "# plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    if len(data['text']) > 10000:\n",
    "        print(data['text'])\n",
    "        print(\"===\" * 20)\n",
    "        print(data['images'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = sum(1 for data in dataset if \"<|image|><|image|>\" in data['text'])\n",
    "print(f\"Number of texts containing '<|image|><|image|>': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = dataset[0]['text'].count('<|image|>')\n",
    "print(f\"Number of image tokens : {image_count}\")\n",
    "print(f\"Number of downloaded images: {len(dataset[0]['images'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    if len(data['images']) != data['text'].count('<|image|>'):\n",
    "        print(f\"Mismatch in data: {data['text']}\")\n",
    "        print(f\"Expected images: {data['text'].count('<|image|>')}, Found images: {len(data['images'])}\")\n",
    "        break\n",
    "print(\"All data entries have matching image counts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "print(dataset[0]['text'])\n",
    "print(dataset[0]['images'])\n",
    "print(dataset[0]['source_data'])\n",
    "image0 = Image.open(\"/fs/archive/share/mm_datasets/obelics_images/0.jpg\")\n",
    "image1 = Image.open(\"/fs/archive/share/mm_datasets/obelics_images/1.jpg\")\n",
    "image2 = Image.open(\"/fs/archive/share/mm_datasets/obelics_images/2.jpg\")\n",
    "iamge3 = Image.open(\"/fs/archive/share/mm_datasets/obelics_images/3.jpg\")\n",
    "\n",
    "display(image0)\n",
    "display(image1)\n",
    "display(image2)\n",
    "display(iamge3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 0\n",
    "for data in dataset:\n",
    "    image_number += len(data['images'])\n",
    "print(f\"Total number of images in the dataset: {image_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pillow_avif\n",
    "from PIL import Image\n",
    "\n",
    "filtered_dataset = []\n",
    "for data in dataset:\n",
    "    all_images_valid = True\n",
    "    for image in data['images']:\n",
    "        try:\n",
    "            img = Image.open(f\"/fs/archive/share/mm_datasets/obelics_images/{image}\")\n",
    "            img.verify()\n",
    "        except Exception as e:\n",
    "            all_images_valid = False\n",
    "            break\n",
    "    if all_images_valid:\n",
    "        filtered_dataset.append(data)\n",
    "\n",
    "dataset = filtered_dataset\n",
    "print(f\"Filtered dataset length: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = [data for data in dataset if len(data['text']) < 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_lengths = [len(data['images']) for data in sample_dataset]\n",
    "\n",
    "plt.hist(image_lengths, bins=range(max(image_lengths) + 2), alpha=0.75, align='left')\n",
    "plt.xlabel('Number of Images')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Number of Images in sample_dataset')\n",
    "plt.xticks(range(max(image_lengths) + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"/fs/archive/share/Qwen2.5-VL-7B-Instruct\",\n",
    "    use_fast=True)\n",
    "tokenizer: PreTrainedTokenizerBase = processor.tokenizer\n",
    "\n",
    "special_token = \"<|image|>\"\n",
    "if special_token not in tokenizer.get_vocab():\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [special_token]})\n",
    "\n",
    "def chunk_dataset_by_token_length(dataset, max_tokens=200):\n",
    "    new_dataset = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        text = entry[\"text\"]\n",
    "        images = entry[\"images\"]\n",
    "\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        current_chunk = []\n",
    "        current_image_count = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            current_chunk.append(token)\n",
    "            if token == special_token:\n",
    "                current_image_count += 1\n",
    "\n",
    "            if len(current_chunk) >= max_tokens:\n",
    "                chunk_text = tokenizer.convert_tokens_to_string(current_chunk)\n",
    "                chunk_images = images[:current_image_count]\n",
    "                images = images[current_image_count:]\n",
    "                new_dataset.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"images\": chunk_images\n",
    "                })\n",
    "\n",
    "                current_chunk = []\n",
    "                current_image_count = 0\n",
    "\n",
    "        # 最后一段\n",
    "        if current_chunk:\n",
    "            chunk_text = tokenizer.convert_tokens_to_string(current_chunk)\n",
    "            chunk_images = images[:current_image_count]\n",
    "            new_dataset.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"images\": chunk_images\n",
    "            })\n",
    "\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_dataset[0]['text'])\n",
    "print(sample_dataset[0]['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = [sample_dataset[0]]\n",
    "test_chunked_dataset = chunk_dataset_by_token_length(test_dataset, max_tokens=200)\n",
    "print(len(test_dataset[0]['images']))\n",
    "print(len(test_chunked_dataset))\n",
    "print(test_chunked_dataset[0]['text'])\n",
    "print(test_chunked_dataset[0]['images'])\n",
    "print(test_chunked_dataset[1]['text'])\n",
    "print(test_chunked_dataset[1]['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_dataset = chunk_dataset_by_token_length(sample_dataset, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunked_dataset[0]['text'])\n",
    "print(chunked_dataset[0]['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_dataset = [data for data in chunked_dataset if len(data['images']) <= 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved to obelics_chunked_dataset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/fs/archive/share/mm_datasets/obelics_chunked_dataset.json\", \"w\") as f:\n",
    "    json.dump(chunked_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Truncated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/fs/archive/share/mm_datasets/obelics_chunked_dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = 0\n",
    "for data in dataset:\n",
    "    if len(data['images']) > max_images:\n",
    "        max_images = len(data['images'])\n",
    "print(max_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    if len(data['images']) == 30:\n",
    "        print(data['text'])\n",
    "        print(\"===\" * 20)\n",
    "        print(data['images'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"/fs/archive/share/mm_datasets/obelics_images/12434.jpg\")\n",
    "display(image)\n",
    "width, height = image.size\n",
    "print(f\"Width: {width}, Height: {height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len([data for data in dataset if len(data['images']) <= 5])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "IMAGE_FACTOR = 28\n",
    "MIN_PIXELS = 4 * 28 * 28\n",
    "MAX_PIXELS = 1024 * 28 * 28\n",
    "MAX_RATIO = 200\n",
    "\n",
    "def round_by_factor(number: int, factor: int) -> int:\n",
    "    \"\"\"Returns the closest integer to 'number' that is divisible by 'factor'.\"\"\"\n",
    "    return round(number / factor) * factor\n",
    "\n",
    "\n",
    "def ceil_by_factor(number: int, factor: int) -> int:\n",
    "    \"\"\"Returns the smallest integer greater than or equal to 'number' that is divisible by 'factor'.\"\"\"\n",
    "    return math.ceil(number / factor) * factor\n",
    "\n",
    "\n",
    "def floor_by_factor(number: int, factor: int) -> int:\n",
    "    \"\"\"Returns the largest integer less than or equal to 'number' that is divisible by 'factor'.\"\"\"\n",
    "    return math.floor(number / factor) * factor\n",
    "\n",
    "def smart_resize(\n",
    "    height: int, width: int, \n",
    "    factor: int = IMAGE_FACTOR, \n",
    "    min_pixels: int = MIN_PIXELS, \n",
    "    max_pixels: int = MAX_PIXELS\n",
    ") -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Rescales the image so that the following conditions are met:\n",
    "        1. Both dimensions (height and width) are divisible by 'factor'.\n",
    "        2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n",
    "        3. The aspect ratio of the image is maintained as closely as possible.\n",
    "    \"\"\"\n",
    "    if max(height, width) / min(height, width) > MAX_RATIO:\n",
    "        raise ValueError(\n",
    "            f\"absolute aspect ratio must be smaller than {MAX_RATIO}, got {max(height, width) / min(height, width)}\"\n",
    "        )\n",
    "    h_bar = max(factor, round_by_factor(height, factor))\n",
    "    w_bar = max(factor, round_by_factor(width, factor))\n",
    "    if h_bar * w_bar > max_pixels:\n",
    "        beta = math.sqrt((height * width) / max_pixels)\n",
    "        h_bar = floor_by_factor(height / beta, factor)\n",
    "        w_bar = floor_by_factor(width / beta, factor)\n",
    "    elif h_bar * w_bar < min_pixels:\n",
    "        beta = math.sqrt(min_pixels / (height * width))\n",
    "        h_bar = ceil_by_factor(height * beta, factor)\n",
    "        w_bar = ceil_by_factor(width * beta, factor)\n",
    "    return h_bar, w_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for data in dataset:\n",
    "    for image_path in data['images']:\n",
    "        image = Image.open(os.path.join(\"/fs/archive/share/mm_datasets/obelics_images\", image_path))\n",
    "        resized_height, resized_width = smart_resize(\n",
    "            image.height, image.width\n",
    "        )\n",
    "        try:\n",
    "            image.verify()  # Verify the image is not corrupted\n",
    "            # image = image.resize((resized_width, resized_height))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping corrupted image: {image_path}, error: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "i = Image.open(\"/fs/archive/share/mm_datasets/obelics_images/373.jpg\")\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "i = i.resize((224, 224))\n",
    "display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "import pillow_avif\n",
    "\n",
    "image_dir = \"/fs/archive/share/mm_datasets/obelics_images\"\n",
    "avif_count = 0\n",
    "truncated_count = 0\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = False  # Reset to default behavior\n",
    "\n",
    "for image_name in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        img.verify()  # Verify the image is not corrupted\n",
    "    except OSError as e:\n",
    "        if \"AVIF\" in str(e):\n",
    "            avif_count += 1\n",
    "        elif \"Truncated\" in str(e):\n",
    "            truncated_count += 1\n",
    "\n",
    "print(f\"Number of images requiring pillow_avif: {avif_count}\")\n",
    "print(f\"Number of images requiring ImageFile.LOAD_TRUNCATED_IMAGES: {truncated_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"qa_flattened.json\", \"rb\") as f:\n",
    "    flattened_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for item in flattened_data:\n",
    "    a = item[\"qry\"].count(\"<|image|>\")\n",
    "    b = len(item[\"qry_image_path\"])\n",
    "    if a != b:\n",
    "        print(f\"Mismatch found in item: {item['qry']}\")\n",
    "        print(f\"Count in text: {a}, Count in image paths: {b}\")\n",
    "        count += 1\n",
    "print(f\"Total mismatches found: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
