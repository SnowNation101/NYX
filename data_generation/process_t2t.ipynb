{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read: 2wikimultihopqa/train_with_retrieved_docs.jsonl\n",
      "Successfully loaded 15000 items from 2wikimultihopqa/train_with_retrieved_docs.jsonl\n",
      "Attempting to read: 2wikimultihopqa/dev_with_retrieved_docs.jsonl\n",
      "Successfully loaded 12576 items from 2wikimultihopqa/dev_with_retrieved_docs.jsonl\n",
      "Attempting to read: bamboogle/test_with_retrieved_docs.jsonl\n",
      "Successfully loaded 125 items from bamboogle/test_with_retrieved_docs.jsonl\n",
      "Attempting to read: hotpotqa/train_with_retrieved_docs.jsonl\n",
      "Successfully loaded 90447 items from hotpotqa/train_with_retrieved_docs.jsonl\n",
      "Attempting to read: hotpotqa/dev_with_retrieved_docs.jsonl\n",
      "Successfully loaded 7405 items from hotpotqa/dev_with_retrieved_docs.jsonl\n",
      "Attempting to read: musique/train_with_retrieved_docs.jsonl\n",
      "Successfully loaded 19938 items from musique/train_with_retrieved_docs.jsonl\n",
      "Attempting to read: musique/dev_with_retrieved_docs.jsonl\n",
      "Successfully loaded 2417 items from musique/dev_with_retrieved_docs.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data_dir = \"/fs/archive/share/mm_datasets/t2t_data\"\n",
    "\n",
    "SUBSET_SPLIT = {\n",
    "    \"2wikimultihopqa\": ['train', 'dev'],\n",
    "    \"bamboogle\": ['test'],\n",
    "    \"hotpotqa\": ['train', 'dev'],\n",
    "    \"musique\": ['train', 'dev'],\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for dataset_name, splits in SUBSET_SPLIT.items():\n",
    "    if dataset_name not in datasets:\n",
    "        datasets[dataset_name] = {}\n",
    "\n",
    "    for split in splits:\n",
    "        dataset_path = f\"{dataset_name}/{split}_with_retrieved_docs.jsonl\"\n",
    "        print(f\"Attempting to read: {dataset_path}\")\n",
    "\n",
    "        data_from_file = []\n",
    "        with open(os.path.join(data_dir, dataset_path), 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data_from_file.append(json.loads(line.strip()))\n",
    "        \n",
    "        datasets[dataset_name][split] = data_from_file\n",
    "\n",
    "        print(f\"Successfully loaded {len(data_from_file)} items from {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 15000 items for 2wikimultihopqa - train\n",
      "Processed 12576 items for 2wikimultihopqa - dev\n",
      "==================================================\n",
      "Processed 125 items for bamboogle - test\n",
      "==================================================\n",
      "Processed 90447 items for hotpotqa - train\n",
      "Processed 7405 items for hotpotqa - dev\n",
      "==================================================\n",
      "Processed 19938 items for musique - train\n",
      "Processed 2417 items for musique - dev\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "processed_data = {}\n",
    "for dataset_name, splits in SUBSET_SPLIT.items():\n",
    "    for split in splits:\n",
    "        new_split_data = []\n",
    "        if dataset_name not in processed_data:\n",
    "            processed_data[dataset_name] = {}\n",
    "        for item in datasets[dataset_name][split]:\n",
    "            new_item = {\n",
    "                \"qry\": item[\"question\"],\n",
    "                \"qry_image_path\": [],\n",
    "                \"pos_text\": item[\"retrieved_docs\"][0],\n",
    "                \"pos_image_path\": [],\n",
    "                \"neg_text\": item[\"retrieved_docs\"][10:],\n",
    "                \"neg_image_path\": [],\n",
    "                \"ans\": item[\"golden_answers\"],\n",
    "            }\n",
    "            new_split_data.append(new_item)\n",
    "        processed_data[dataset_name][split] = new_split_data\n",
    "        print(f\"Processed {len(new_split_data)} items for {dataset_name} - {split}\")\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
